{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "e1f3d86e"
   },
   "source": [
    "In this class, we will talk about model explainability but more in the context of data explainability or root cause analysis. In many cases building a very good machine learning model is not an ultimate goal. What is really wanted is the data understanding. A factory wants to know why the product is plagued with a defect, not to predict afterward if there is a defect or not. A football team wants to know which position is the best for scoring a goal, not what's the probability of scoring from a given position. And even when they want a prediction they would love to see the justification to trust the model. Often a nice plot is worth more than sophisticated machine-learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "0cf435ee"
   },
   "outputs": [],
   "source": [
    "import dalex as dx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "ac5edb05"
   },
   "outputs": [],
   "source": [
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "18835b4c"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = data[\"target\"]\n",
    "df[\"target\"] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "521043b8"
   },
   "source": [
    "You should already be familiar with many data visualization techniques so we will not train it now. I just want to share a less popular type of data analysis. Usually plotting the target against any feature is not helpful but after some modification, we might be able to see some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "eae45f14"
   },
   "outputs": [],
   "source": [
    "plt.plot(df.flavanoids, y, \"bo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "21d4d36d"
   },
   "source": [
    "For each value, we can plot the average target for data:\n",
    " - below that value\n",
    " - above that value\n",
    " - around that value\n",
    "\n",
    "Please note that for the line \"above that value\" the more left we go the higher fraction of data is covered. The same with the \"below that value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "2787dead9"
   },
   "outputs": [],
   "source": [
    "for col in df.columns.drop(\"target\"):\n",
    "    tmp = df.sort_values(col)\n",
    "    plt.title(col)\n",
    "    plt.plot(\n",
    "        tmp[col],\n",
    "        tmp[col].apply(lambda x: tmp[tmp[col] <= x].target.mean()),\n",
    "        label=\"<=\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        tmp[col],\n",
    "        tmp[col].apply(lambda x: tmp[tmp[col] >= x].target.mean()),\n",
    "        label=\">=\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        tmp[col],\n",
    "        np.convolve(np.ones(20) / 20, tmp.target, mode=\"same\"),\n",
    "        label=\"~=~\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "4bffc8a5"
   },
   "source": [
    "Ok, let's just train a model. We are not interested in top performance right now so we will skip hyperparameter optimization. Also, we want to find the pattern in the data we have, so we don't split the data into validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "0544ef6c"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "x = df.drop(\"target\", axis=1)\n",
    "y = df.target\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "db2868fd"
   },
   "outputs": [],
   "source": [
    "plt.plot(df.target, model.predict(x), \"bo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "564d4ab7"
   },
   "source": [
    "Dalex is a python package for model explainability. We will use some of its functions to understand the data and the model better. First, we need to create an explainer model. Since we are not interested in checking the model performance but the relation between the data and the target we will use the whole dataset here. In the first case, we might want to use the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "4522325a"
   },
   "outputs": [],
   "source": [
    "exp = dx.Explainer(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "f0296fb6"
   },
   "outputs": [],
   "source": [
    "fi = exp.model_parts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "cd7d248a"
   },
   "source": [
    "The first step will be feature importance. It's a basic analysis where we calculate the global impact of a feature. The idea in dalex default approach is to measure how much the model performance is worsening after removing this feature. Of course, it would require retraining the model, the optimal set of hyperparameters might be different and it might affect the results. To avoid these problems we do not retrain the model. Instead, we simulate its removal by assigning random values to it. To make it more realistic the values are not completely random, we just shuffle this column in a dataframe, do the prediction, check performance and repeat these steps multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "09f500ae"
   },
   "outputs": [],
   "source": [
    "fi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "b7f9535a"
   },
   "source": [
    "Another useful tool is a partial dependency plot. For a given feature we observe what's the average output of our model for different values of this feature. For each considered value we set this value for each row in our dataframe and calculate an average prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "3a20223d"
   },
   "outputs": [],
   "source": [
    "exp.model_profile().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "94a61afa"
   },
   "source": [
    "\n",
    "We can also create similar plots for single rows. Here for each column, we present what would be the output from the model assuming we keep all remaining values and change the value of this one selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "b43ed2d2"
   },
   "outputs": [],
   "source": [
    "exp.predict_profile(x.iloc[[15, 80]]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "1f7ef147"
   },
   "source": [
    "SHAP values are equivalents of Shapley values for the predictive models. It estimates the effect of a particular value of a particular feature for a prediction of a considered row. It's also done by replacing this value with proper sampling and replacing this value and measuring the effect on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "0e417ef7"
   },
   "outputs": [],
   "source": [
    "exp.predict_parts(x.iloc[15], type=\"shap\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "1c8198c3"
   },
   "outputs": [],
   "source": [
    "exp.predict_parts(x.iloc[15], type=\"shap\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "81dc4fb9"
   },
   "source": [
    "The result is based on sampling so the result for the same row can vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "a9dd9aef"
   },
   "outputs": [],
   "source": [
    "exp.predict_parts(x.iloc[88], type=\"shap\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "data295a9"
   },
   "outputs": [],
   "source": [
    "exp.predict_parts(x.iloc[88], type=\"shap\").result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "4d3af9bd"
   },
   "source": [
    "**Task** For each class find the most representative examples and plot breakdown for them.\n",
    "\n",
    "Imagine we have a model classifying dogs and cats. Then a good example would be to show e.g. 3 breeds of dogs and the same with cats. Showing 5 golden retrievers although cute is not the best approach.\n",
    "\n",
    "There isn't a single best way how to approach this task. There are many good solutions. Think about what you want to achieve and then how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Generator\n",
    "\n",
    "import seaborn as sns\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "015c6cef"
   },
   "outputs": [],
   "source": [
    "DistanceFn = Callable[[np.ndarray, np.ndarray], np.matrix]\n",
    "\n",
    "\n",
    "def add_column(df: pd.DataFrame, column: ArrayLike, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Add column to the dataframe. Best works with `pd.DataFrame.pipe`\n",
    "    method.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe to extend\n",
    "        column: Column to add\n",
    "        name: Name of the column\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with added column\n",
    "    \"\"\"  # noqa: D205\n",
    "    return pd.concat(\n",
    "        [df.reset_index(), pd.DataFrame({name: column})], axis=1\n",
    "    ).set_index(\"index\")\n",
    "\n",
    "\n",
    "def annotate_representatives(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_strategy: Any,\n",
    "    distance: DistanceFn,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Divide into clusters and for each pick representative closest to\n",
    "    the centroid.\n",
    "\n",
    "    Args:\n",
    "        df: Data for picking representatives from.\n",
    "        cluster_strategy: Clustering algorithm.\n",
    "        distance: Distance function.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with additional column: booleans signifying whether the\n",
    "        record is representative.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Cluster\n",
    "    cluster_strategy = cluster_strategy.fit(df)\n",
    "    centroids = cluster_strategy.cluster_centers_\n",
    "    labels = cluster_strategy.labels_\n",
    "\n",
    "    # Distance from each record to each centroid, stored in\n",
    "    # (N_records, N_centroids) shape\n",
    "    distances_mat = distance(df.to_numpy(), centroids)\n",
    "\n",
    "    result = df.pipe(add_column, column=labels, name=\"cluster\")\n",
    "    result[\"repr\"] = False\n",
    "    ids = []\n",
    "\n",
    "    # Can't \"groupby\" because the same records might occur => exclude them\n",
    "    # after each iter\n",
    "\n",
    "    # Iterate by columns (distances corresponding to a single centroid)\n",
    "    for distances in distances_mat.T:\n",
    "        ids.extend(\n",
    "            result.pipe(add_column, column=distances, name=\"distance\")\n",
    "            .drop(ids)\n",
    "            .sort_values(\"distance\")\n",
    "            .head(1)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "    result.loc[pd.Index(ids), \"repr\"] = True\n",
    "    return result.sort_values(\"index\")\n",
    "\n",
    "\n",
    "def annotate_by_classes(\n",
    "    data: pd.DataFrame,\n",
    "    n_representatives: int,\n",
    "    target_feature: str,\n",
    "    distance: DistanceFn = euclidean_distances,\n",
    "    random_state: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    return pd.concat([\n",
    "        annotate_representatives(\n",
    "            cls_data,\n",
    "            KMeans(n_representatives, n_init=\"auto\"),\n",
    "            euclidean_distances,\n",
    "        )\n",
    "        for cls, cls_data in data.groupby(target_feature)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "85fa1740"
   },
   "outputs": [],
   "source": [
    "# Annotate representatives for each class\n",
    "result = annotate_by_classes(\n",
    "    data=df, n_representatives=5, target_feature=\"target\", random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "# Perform dim reduction and prepare data\n",
    "plot_data = PCA().fit_transform(\n",
    "    result.drop([\"target\", \"cluster\", \"repr\"], axis=1)\n",
    ")\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "plot_data[\"target\"] = result[\"target\"]\n",
    "plot_data[\"cluster\"] = result[\"cluster\"]\n",
    "plot_data[\"repr\"] = result[\"repr\"]\n",
    "\n",
    "# Plot for all classes\n",
    "plt.tick_params(labelbottom=False, labelleft=False)\n",
    "plt.title(\"All classes\")\n",
    "sns.scatterplot(data=plot_data, x=0, y=1, hue=\"target\", palette=\"deep\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for each class\n",
    "clss = result[\"target\"].unique()\n",
    "palette = sns.color_palette(\"deep\")\n",
    "for cls, color in zip(clss, palette):\n",
    "    # Plotting params\n",
    "    plt.title(f\"Class {cls}\")\n",
    "    plt.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "    # Plot points\n",
    "    sns.scatterplot(data=plot_data, x=0, y=1, color=\"gray\")\n",
    "    sns.scatterplot(\n",
    "        data=plot_data[plot_data[\"target\"] == cls],\n",
    "        x=0,\n",
    "        y=1,\n",
    "        color=color + (0.5,),\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        data=plot_data[(plot_data[\"target\"] == cls) & plot_data[\"repr\"]],\n",
    "        x=0,\n",
    "        y=1,\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "d9736904"
   },
   "source": [
    "There are other approaches that can be used for model explainability.\n",
    " - LIME - approximating model locally by a linear model\n",
    " - Anchor - approximating model locally by a rule-based model\n",
    " - Prototype - justifying a new prediction by showing a similar example from the data (a prototype)\n",
    " - Counterfactual Explanation - showing a similar example from the dataset with a different prediction to show what must be changed to change the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "5d7494d7"
   },
   "source": [
    "# Task\n",
    "\n",
    "- take a dataset you want\n",
    "- perform an exploratory data analysis (data visualization)\n",
    "- create a sklearn pipeline for data preprocessing\n",
    "- add new features (one hot encoding for example)\n",
    "- add predictive model as the last step of the pipeline\n",
    "- prepare a report with model explainability\n",
    "\n",
    "Send it to gmiebs@cs.put.poznan.pl within 144 hours after the class is finished. Start the subject of the email with [IR]\n",
    "\n",
    "Assume your report will be read by a domain expert from the area of the data, in our case a wine expert, without any computer science / data science skills. It means the person will not get much from raw plots and diagrams. Everything has to be explained to be understood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Feature Descriptions\n",
    "* `person_age`: Applicant’s age in years.\n",
    "* `person_home_ownership`: Status of homeownership (e.g., Rent, Own, Mortgage).\n",
    "* `person_income`: Annual income of the applicant in USD.\n",
    "* `person_emp_length`: Length of employment in years.\n",
    "* `loan_intent`: Purpose of the loan (e.g., Education, Medical, Personal).\n",
    "* `loan_grade`: Risk grade assigned to the loan, assessing the applicant’s creditworthiness.\n",
    "* `loan_amnt`: Total loan amount requested by the applicant.\n",
    "* `loan_int_rate`: Interest rate associated with the loan.\n",
    "* `loan_status`: The approval status of the loan (approved or not approved).\n",
    "* `loan_percent_income`: Percentage of the applicant’s income allocated towards loan repayment.\n",
    "* `cb_person_default_on_file`: Indicates if the applicant has a history of default ('Y' for yes, 'N' for no).\n",
    "* `cb_person_cred_hist_length`: Length of the applicant’s credit history in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "fdabac1d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "df_vis = df.copy()\n",
    "cat_cols = [\n",
    "    \"person_home_ownership\",\n",
    "    \"loan_intent\",\n",
    "    \"loan_grade\",\n",
    "    \"cb_person_default_on_file\",\n",
    "]\n",
    "df_vis[cat_cols] = OrdinalEncoder().fit_transform(df_vis[cat_cols])\n",
    "plt.figure(figsize=(14, 14)).suptitle(\"Feature Correlation\")\n",
    "sns.heatmap(df_vis.corr(), annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "The plot above presents heatmap of for the numerical columns of the dataset. While numbers about zero imply no correlation, positive and negative values represent direct and indirect correlations respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_int_rate_by_loan_grade = df.groupby(\"loan_grade\")[\"loan_int_rate\"].mean()\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First plot (bar chart for mean interest rate by loan grade)\n",
    "ax1.bar(mean_int_rate_by_loan_grade.index, mean_int_rate_by_loan_grade)\n",
    "ax1.set_xlabel(\"Loan Grade\")\n",
    "ax1.set_ylabel(\"Mean Interest Rate\")\n",
    "ax1.set_title(\"Mean Interest Rate by Loan Grade\")\n",
    "\n",
    "# Second plot (violin plot for interest rate distribution by loan grade)\n",
    "ticks, interest_by_rate_data = list(\n",
    "    zip(*[\n",
    "        (cls, cls_data[\"loan_int_rate\"])\n",
    "        for cls, cls_data in df.groupby(\"loan_grade\")\n",
    "    ])\n",
    ")\n",
    "ax2.violinplot(interest_by_rate_data, showmeans=False, showextrema=False)\n",
    "ax2.set_xticks([y + 1 for y in range(len(interest_by_rate_data))], ticks)\n",
    "ax2.set_title(\"Interest Rate Distribution by Loan Grade\")\n",
    "ax2.set_xlabel(\"Loan Grade\")\n",
    "ax2.set_ylabel(\"Interest Rate\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks, default_on_file_by_grade_data = list(\n",
    "    zip(*[\n",
    "        (cls, cls_data[\"cb_person_default_on_file\"].value_counts())\n",
    "        for cls, cls_data in df[df[\"loan_status\"] == 1].groupby(\"loan_grade\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "y_N, y_Y = [], []\n",
    "for el in default_on_file_by_grade_data:\n",
    "    el_N = el[\"N\"] if \"N\" in el.index else 0\n",
    "    el_Y = el[\"Y\"] if \"Y\" in el.index else 0\n",
    "    y_N.append(el_N / (el_N + el_Y))\n",
    "    y_Y.append(el_Y / (el_N + el_Y))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(ticks, y_N, color=\"green\")\n",
    "ax.bar(ticks, y_Y, color=\"red\", bottom=y_N)\n",
    "ax.set_xlabel(\"Loan Grade\")\n",
    "ax.set_ylabel(\"% of the people with positive loan status\")\n",
    "ax.set_title(\"How prior default influences future Loan Grade\")\n",
    "plt.legend([\"No prior default\", \"Has prior default\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The plot above shows that those having default history may get new loans starting with grade C and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_vs_positive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    loan_status_both = df.reset_index()\n",
    "    loan_status_positive = df[df[\"loan_status\"] == 1].reset_index()\n",
    "\n",
    "    loan_status_both[\"loan_status\"] = \"all\"\n",
    "    loan_status_positive[\"loan_status\"] = \"positive\"\n",
    "\n",
    "    return pd.concat([loan_status_both, loan_status_positive])\n",
    "\n",
    "\n",
    "def subplots(\n",
    "    df_columns: int, n_cols: int, title: str\n",
    ") -> tuple[plt.Figure, Generator[plt.Axes, str, None]]:\n",
    "    n_rows = (df_columns + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    def axes_gen() -> Generator[plt.Axes, str, None]:\n",
    "        collapse_flag = False\n",
    "        for ax in axes.ravel():\n",
    "            if not collapse_flag:\n",
    "                collapse_flag = (yield ax) == \"collapse\"\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "        yield\n",
    "\n",
    "    return fig, axes_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### Distribution of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_data = (\n",
    "    (temp := all_vs_positive(df))\n",
    "    .select_dtypes(include=\"number\")\n",
    "    .assign(loan_status=temp[\"loan_status\"])\n",
    "    .drop([\"index\", \"id\"], axis=1)\n",
    ")\n",
    "\n",
    "plotting_columns = list(filter(lambda x: x != \"loan_status\", kde_data.columns))\n",
    "_, axes = subplots(\n",
    "    df_columns=len(plotting_columns), n_cols=3, title=\"Numerical features\"\n",
    ")\n",
    "for col, ax in zip(plotting_columns, axes):\n",
    "    ax.tick_params(labelleft=False)\n",
    "    sns.kdeplot(\n",
    "        data=kde_data, x=col, hue=\"loan_status\", ax=ax, fill=True, bw_adjust=2\n",
    "    )\n",
    "axes.send(\"collapse\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = (\n",
    "    (temp := all_vs_positive(df))\n",
    "    .select_dtypes(include=\"object\")\n",
    "    .assign(loan_status=temp[\"loan_status\"])\n",
    "    .assign(loan_intent=temp[\"loan_intent\"].str[:3] + \".\")\n",
    ")\n",
    "\n",
    "plotting_columns = list(\n",
    "    filter(lambda x: x != \"loan_status\", hist_data.columns)\n",
    ")\n",
    "fig, axes = subplots(\n",
    "    df_columns=len(plotting_columns), n_cols=2, title=\"Categorical features\"\n",
    ")\n",
    "for col, ax in zip(plotting_columns, axes):\n",
    "    total_value_counts = hist_data[hist_data[\"loan_status\"] == \"all\"][\n",
    "        col\n",
    "    ].value_counts()\n",
    "    positive_value_counts = hist_data[hist_data[\"loan_status\"] == \"positive\"][\n",
    "        col\n",
    "    ].value_counts()\n",
    "\n",
    "    positive_value_counts = (\n",
    "        (positive_value_counts / total_value_counts * 100)\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "    )\n",
    "    total_value_counts = total_value_counts.reset_index().assign(count=100)\n",
    "\n",
    "    if col != \"loan_grade\":\n",
    "        order = positive_value_counts.sort_values(\"count\")[col]\n",
    "    else:\n",
    "        order = positive_value_counts[col]\n",
    "\n",
    "    sns.barplot(\n",
    "        data=total_value_counts,\n",
    "        y=col,\n",
    "        x=\"count\",\n",
    "        ax=ax,\n",
    "        color=\"gray\",\n",
    "        order=order,\n",
    "    )\n",
    "    sns.barplot(\n",
    "        data=positive_value_counts, y=col, x=\"count\", ax=ax, order=order\n",
    "    )\n",
    "axes.send(\"collapse\")\n",
    "\n",
    "fig.set_figwidth(20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### 3, 4, 5. Preprocessing, new features, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "X = df.drop([\"loan_status\"], axis=1).set_index(\"id\")\n",
    "y = df[\"loan_status\"]\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "numerical_categorical_transformer = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(), cat_cols),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", numerical_categorical_transformer),\n",
    "    (\"clf\", HistGradientBoostingClassifier(max_iter=1000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 6. Explanations\n",
    "\n",
    "\n",
    "In this report we will explore how the classifier is making its classification. We will use the following techniques of explain-ability:\n",
    "* `Variable importance` (on the whole dataset)\n",
    "* `Classification depending on a particular feature` (on particular examples)\n",
    "* `Shapley values` (on particular examples)\n",
    "* `LIME` (on particular examples)\n",
    "* `Anchor` (on particular examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Selecting class representatives\n",
    "\n",
    "By the virtue of the fact that considerable portion of our report is dedicated to explaining the model behavior for a given record, it is important to pick the most interesting ones. From analytical perspective, class representatives are the most insightful.\n",
    "\n",
    "These representatives are identified by grouping model predictions as follows: for each class (i.e. loan denied and loan approved) 3 groups are created based on the similarity between records. The record closest to the center of each group is then chosen as the class representative.\n",
    "\n",
    "As a result, each class would have 3 representative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_X = pipe.named_steps[\"preprocessing\"].transform(X)\n",
    "ndarray_y = np.expand_dims(y.to_numpy(), axis=1)\n",
    "columns = (\n",
    "    num_cols.tolist()\n",
    "    + pipe.named_steps[\"preprocessing\"]\n",
    "    .named_transformers_[\"cat\"]\n",
    "    .get_feature_names_out()\n",
    "    .tolist()\n",
    "    + [\"loan_status\"]\n",
    ")\n",
    "df_preprocessed = pd.DataFrame(\n",
    "    np.hstack([ndarray_X, ndarray_y]), columns=columns\n",
    ")\n",
    "df_with_representatives = annotate_by_classes(\n",
    "    data=df_preprocessed,\n",
    "    n_representatives=3,\n",
    "    target_feature=\"loan_status\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "loan_negative_representatives = df_with_representatives[\n",
    "    df_with_representatives[\"repr\"]\n",
    "    & (df_with_representatives[\"loan_status\"] == 0)\n",
    "].index\n",
    "loan_positive_representatives = df_with_representatives[\n",
    "    df_with_representatives[\"repr\"]\n",
    "    & (df_with_representatives[\"loan_status\"] == 1)\n",
    "].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Variable importance\n",
    "\n",
    "The plot below presents variable importance (VI) on the dataset. Variables with higher VI have more influence on what is the classification of the examples. VI is calculated based on the dropout loss, showing how the accuracy of the classification would lower in case the variable wasn't considered during classification.\n",
    "We see that `loan_percent_income` is the variable with the highest importance and dropping it would lower the classification accuracy by about 8%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = dx.Explainer(pipe, X, y)\n",
    "fi = exp.model_parts()\n",
    "fi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Classification depending on a particular feature\n",
    "\n",
    "The plot below shows how the classification result of a particular instance from the dataset would change when changing the value of the given feature. Generally speaking, it shows that, for instance, if the loan percent income is 20% then chances of getting a loan are close to 0, however, when increased to 40%, the classification result would be positive with about 80% probability. Keep in mind that all other feature values remain unaltered.\n",
    "\n",
    "For the sake of lesser space consumption, multiple instances are shown on the same plots. The first set of plots concerns examples with classification decision of denying the loan and the second set contains examples with positive classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict_profile(X.loc[loan_negative_representatives]).plot(\n",
    "    title=\"Loan denied\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict_profile(X.loc[loan_positive_representatives]).plot(\n",
    "    title=\"Loan approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Shapley values\n",
    "\n",
    "The plot with `Shapley values` shows how particular features of the considered instance influenced its classification result. The bars show how much the feature pushed the decision in the direction of either approving the loan or denying it. Red bars represent denial while green bars show the approval. For clarity, the features are plotted in the descending order of their importance.\n",
    "\n",
    "In this section and in the consecutive ones we will have explain-ability plots for the particular instances. We've chosen them as the best representatives of the decision class, however, distant from each other. The first set of plots concerns examples with classification decision of denying the loan and the second set contains examples with positive classification.\n",
    "\n",
    "**Note:** In this and in the following sections we are analyzing instances from already preprocessed data. That means you can encounter new feature names and scaled values for features:\n",
    "1. We use one-hot-encoding for the categorical features meaning that you may notice that some features are not from the list presented earlier. One-hot-encoded features are listed bellow:\n",
    "    * `person_home_ownership` -> ex: `person_home_ownership_RENT`, meaning that person is renting a house\n",
    "    * `loan_intent` -> ex: `loan_intent_MEDICAL`\n",
    "    * `loan_grade` -> ex: `loan_grade_A` (grades are from A to G)\n",
    "    * `cb_person_default_on_file` -> either `cb_person_default_on_file_Y` (has prior default) or `cb_person_default_on_file_N` otherwise\n",
    "\n",
    "2. Numerical values are scaled in order to improve classification accuracy (for values with larger scale not to distort the classification algorithm). The values for one-hot-encoded features are either 0 or 1, meaning negative and positive value respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from dalex.predict_explanations import Shap\n",
    "\n",
    "\n",
    "def shapley_batch(\n",
    "    data: pd.DataFrame,\n",
    "    exp: dx.Explainer,\n",
    "    batch: Iterable[int],\n",
    "    random_state: int | None = None,\n",
    ") -> list[Shap]:\n",
    "    return [\n",
    "        exp.predict_parts(\n",
    "            data.loc[idx], type=\"shap\", random_state=random_state\n",
    "        )\n",
    "        for idx in batch\n",
    "    ]\n",
    "\n",
    "\n",
    "def shapley_plot(predicted: list[Shap]) -> None:\n",
    "    predicted[0].plot(objects=predicted[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_negative_shap = shapley_batch(\n",
    "    X, exp, loan_negative_representatives, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_positive_shap = shapley_batch(\n",
    "    X, exp, loan_positive_representatives, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_plot(loan_negative_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_plot(loan_positive_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "#### LIME\n",
    "\n",
    "`Local Interpretable Model-agnostic Explanations (LIME)` provides us with similar information as `Shapley values`, however, it has different underlying mathematical ideas.\n",
    "\n",
    "Briefly, LIME is exploring classification results of the model in the neighborhood of a given instance. In doing so it calculates how much the feature values push the decision in the direction of either approving the loan or denying it. Blue bars represent denial while orange bars show the approval.\n",
    "\n",
    "Probabilities on the right from the graph show the certainty with which the classifier assigned corresponding target value.\n",
    "\n",
    "On the left from the graph one can see feature values (in the data after the preprocessing).\n",
    "\n",
    "**Note:** only the most influential features are presented in LIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.explanation import Explanation as LimeExplanation\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "X_preprocessed = pipe.named_steps[\"preprocessing\"].transform(X)\n",
    "exp = LimeTabularExplainer(\n",
    "    X_preprocessed,\n",
    "    feature_names=num_cols.tolist()\n",
    "    + pipe.named_steps[\"preprocessing\"]\n",
    "    .named_transformers_[\"cat\"]\n",
    "    .get_feature_names_out()\n",
    "    .tolist(),\n",
    "    class_names=[\"negative\", \"positive\"],\n",
    "    discretize_continuous=False,\n",
    "    verbose=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_batch(\n",
    "    data: pd.DataFrame,\n",
    "    exp: LimeTabularExplainer,\n",
    "    batch: Iterable[int],\n",
    "    model: Any,\n",
    ") -> list[LimeExplanation]:\n",
    "    return [\n",
    "        exp.explain_instance(data[idx], model.predict_proba) for idx in batch\n",
    "    ]\n",
    "\n",
    "\n",
    "def lime_plot(explained: list[LimeExplanation]) -> None:\n",
    "    for lime in explained:\n",
    "        lime.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_negative_lime = lime_batch(\n",
    "    X_preprocessed, exp, loan_negative_representatives, pipe.named_steps[\"clf\"]\n",
    ")\n",
    "lime_plot(loan_negative_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_positive_lime = lime_batch(\n",
    "    X_preprocessed, exp, loan_positive_representatives, pipe.named_steps[\"clf\"]\n",
    ")\n",
    "lime_plot(loan_positive_lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Anchor\n",
    "\n",
    "`Anchor`, similarly to `LIME`, explores the neighborhood of the particular instance from the dataset, slightly perturbing the values. As a result, we get set of rules that lead to a certain probability of the correct classification. That's a benefit of anchor - it describes not only the influence of a particular value (e.g. if age = 25 then ...), but constructs the rules that are easy to interpret (e.g. if age > 25 then ...).\n",
    "\n",
    "On the plots below we present 3 representatives for each class (the same way we did in above plots). The first 3 representatives are from the class with denied loan applications and later 3 - with obtained loan. The value one can see is the probability of being assigned to a corresponding class if the listed rules on the left are satisfied.\n",
    "\n",
    "**Note:** When there are multiple rules one can switch them on and off. In so doing one will obtain new probability of the classification for an objective class (dynamically adjusted for the new set of rules).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor_tabular import AnchorTabularExplainer\n",
    "\n",
    "mapping_col = {\n",
    "    col: X[col].unique()\n",
    "    for i, col in filter(lambda x: x[1] in cat_cols, enumerate(X.columns))\n",
    "}\n",
    "mapping_idx = {\n",
    "    i: X[col].unique()\n",
    "    for i, col in filter(lambda x: x[1] in cat_cols, enumerate(X.columns))\n",
    "}\n",
    "\n",
    "\n",
    "def encode(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    for col, values in mapping_col.items():\n",
    "        local_map = {value: i for i, value in enumerate(values)}\n",
    "        data[col] = data[col].map(local_map)\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode(data: np.ndarray, columns: list[str]) -> pd.DataFrame:\n",
    "    data = pd.DataFrame(data, columns=columns)\n",
    "    for col, values in mapping_col.items():\n",
    "        local_map = {i: value for i, value in enumerate(values)}\n",
    "        data[col] = data[col].map(local_map)\n",
    "    return data\n",
    "\n",
    "\n",
    "def anchor_batch(\n",
    "    data: pd.DataFrame,\n",
    "    exp: AnchorTabularExplainer,\n",
    "    batch: Iterable[int],\n",
    "    model: Any,\n",
    ") -> list[LimeExplanation]:\n",
    "    return [\n",
    "        exp.explain_instance(\n",
    "            encode(data.loc[[idx]]).to_numpy(),\n",
    "            lambda x: model.predict(decode(x, columns=data.columns)),\n",
    "            threshold=0.8,\n",
    "        )\n",
    "        for idx in batch\n",
    "    ]\n",
    "\n",
    "\n",
    "def anchor_plot(explained: list[LimeExplanation]) -> None:\n",
    "    for anchor in explained:\n",
    "        anchor.show_in_notebook(show_table=True)\n",
    "\n",
    "\n",
    "exp = AnchorTabularExplainer(\n",
    "    class_names=y.unique(),\n",
    "    feature_names=X.columns,\n",
    "    train_data=encode(X).to_numpy(),\n",
    "    categorical_names=mapping_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_negative_anchor = anchor_batch(\n",
    "    X, exp, loan_negative_representatives, pipe\n",
    ")\n",
    "anchor_plot(loan_negative_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loan_positive_anchor = anchor_batch(\n",
    "    X, exp, loan_positive_representatives, pipe\n",
    ")\n",
    "anchor_plot(loan_positive_anchor)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
